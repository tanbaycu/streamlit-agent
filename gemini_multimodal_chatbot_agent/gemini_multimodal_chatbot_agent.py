import streamlit as st
import google.generativeai as genai
from PIL import Image
import io
import base64
import json


st.set_page_config(page_title="Tr√≤ chuy·ªán ƒêa ph∆∞∆°ng ti·ªán v·ªõi Gemini", layout="wide")
st.title("Tr√≤ chuy·ªán ƒêa ph∆∞∆°ng ti·ªán v·ªõi Gemini üöÄ")
st.caption("Tr·∫£i nghi·ªám s·ª©c m·∫°nh c·ªßa c√°c m√¥ h√¨nh Gemini m·ªõi nh·∫•t v·ªõi t√πy ch·ªânh n√¢ng cao. üåü")


if "messages" not in st.session_state:
    st.session_state.messages = []
if "image" not in st.session_state:
    st.session_state.image = None
if "model_config" not in st.session_state:
    st.session_state.model_config = {
        "model_name": "gemini-1.5-flash-latest",
        "temperature": 0.7,
        "top_p": 1.0,
        "top_k": 40,
        "max_output_tokens": 2048,
    }
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch v√† th√¢n thi·ªán."


GEMINI_MODELS = [
    "gemini-1.5-flash-latest",
    "gemini-2.0-flash-exp",
    "gemini-1.5-flash-8b",
    "gemini-2.0-pro-exp-02-05",
    "gemini-2.0-flash-lite-preview-02-05"
]


with st.sidebar:
    st.title("C√†i ƒë·∫∑t")
    api_key = st.text_input("Nh·∫≠p Google API Key", type="password")
    
    st.title("T√πy ch·ªânh M√¥ h√¨nh")
    selected_model = st.selectbox("Ch·ªçn m√¥ h√¨nh Gemini", GEMINI_MODELS, index=GEMINI_MODELS.index(st.session_state.model_config["model_name"]))
    st.session_state.model_config["model_name"] = selected_model
    
    st.session_state.model_config["temperature"] = st.slider("ƒê·ªô s√°ng t·∫°o", min_value=0.0, max_value=1.0, value=st.session_state.model_config["temperature"], step=0.1)
    st.session_state.model_config["top_p"] = st.slider("Top P", min_value=0.0, max_value=1.0, value=st.session_state.model_config["top_p"], step=0.1)
    st.session_state.model_config["top_k"] = st.number_input("Top K", min_value=1, max_value=100, value=st.session_state.model_config["top_k"])
    st.session_state.model_config["max_output_tokens"] = st.number_input("S·ªë token t·ªëi ƒëa", min_value=1, max_value=8192, value=st.session_state.model_config["max_output_tokens"])
    
    st.title("T√πy ch·ªânh Prompt")
    st.session_state.system_prompt = st.text_area("System Prompt", value=st.session_state.system_prompt, height=100)
    
    st.title("T·∫£i l√™n H√¨nh ·∫£nh")
    uploaded_file = st.file_uploader("T·∫£i l√™n m·ªôt h√¨nh ·∫£nh...", type=["jpg", "jpeg", "png"])


@st.cache_resource
def load_model(api_key, model_name):
    try:
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model_name=model_name)
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o m√¥ h√¨nh: {str(e)}")
        return None

def get_image_download_link(img, filename, text):
    buffered = io.BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:file/png;base64,{img_str}" download="{filename}">{text}</a>'
    return href

if api_key:
    model = load_model(api_key, st.session_state.model_config["model_name"])
    
    if uploaded_file:
        st.session_state.image = Image.open(uploaded_file)
        st.sidebar.image(st.session_state.image, caption='H√¨nh ·∫£nh ƒë√£ t·∫£i l√™n', use_column_width=True)
        st.sidebar.markdown(get_image_download_link(st.session_state.image, "h√¨nh_·∫£nh_ƒë√£_t·∫£i.png", "T·∫£i xu·ªëng h√¨nh ·∫£nh"), unsafe_allow_html=True)

    
    chat_placeholder = st.container()

    with chat_placeholder:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])


    prompt = st.chat_input("B·∫°n mu·ªën bi·∫øt g√¨?")

    if prompt:
        full_prompt = f"{st.session_state.system_prompt}\n\nNg∆∞·ªùi d√πng: {prompt}\n\nTr·ª£ l√Ω:"
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with chat_placeholder:
            with st.chat_message("user"):
                st.markdown(prompt)
        
        inputs = [full_prompt]
        if st.session_state.image:
            inputs.append(st.session_state.image)

        with st.spinner('ƒêang t·∫°o ph·∫£n h·ªìi...'):
            try:
                response = model.generate_content(
                    inputs,
                    generation_config=genai.types.GenerationConfig(
                        temperature=st.session_state.model_config["temperature"],
                        top_p=st.session_state.model_config["top_p"],
                        top_k=st.session_state.model_config["top_k"],
                        max_output_tokens=st.session_state.model_config["max_output_tokens"],
                    )
                )
                st.session_state.messages.append({"role": "assistant", "content": response.text})
                
                with chat_placeholder:
                    with st.chat_message("assistant"):
                        st.markdown(response.text)
            except Exception as e:
                st.error(f"L·ªói t·∫°o ph·∫£n h·ªìi: {str(e)}")

    if st.session_state.image and not prompt:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi ƒë·ªÉ ƒëi k√®m v·ªõi h√¨nh ·∫£nh.")

else:
    st.warning("Vui l√≤ng nh·∫≠p Google API Key c·ªßa b·∫°n ·ªü thanh b√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán.")


if st.button("X√≥a L·ªãch s·ª≠ Tr√≤ chuy·ªán"):
    st.session_state.messages = []
    st.session_state.image = None
    st.rerun()


if st.button("Xu·∫•t L·ªãch s·ª≠ Tr√≤ chuy·ªán"):
    chat_history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.messages])
    st.download_button(
        label="T·∫£i xu·ªëng L·ªãch s·ª≠ Tr√≤ chuy·ªán",
        data=chat_history,
        file_name="lich_su_tro_chuyen.txt",
        mime="text/plain"
    )


st.sidebar.title("Th√¥ng tin M√¥ h√¨nh")
st.sidebar.info(f"""
- M√¥ h√¨nh: {st.session_state.model_config['model_name']}
- ƒê·ªô s√°ng t·∫°o: {st.session_state.model_config['temperature']:.2f}
- Top P: {st.session_state.model_config['top_p']:.2f}
- Top K: {st.session_state.model_config['top_k']}
- S·ªë token t·ªëi ƒëa: {st.session_state.model_config['max_output_tokens']}
- S·ªë l∆∞·ª£ng tin nh·∫Øn: {len(st.session_state.messages)}
""")


if st.sidebar.button("Xu·∫•t C·∫•u h√¨nh"):
    config_data = {
        "model_config": st.session_state.model_config,
        "system_prompt": st.session_state.system_prompt
    }
    st.sidebar.download_button(
        label="T·∫£i xu·ªëng C·∫•u h√¨nh",
        data=json.dumps(config_data, indent=2),
        file_name="cau_hinh_mo_hinh.json",
        mime="application/json"
    )


uploaded_config = st.sidebar.file_uploader("T·∫£i l√™n C·∫•u h√¨nh", type=["json"])
if uploaded_config:
    config_data = json.load(uploaded_config)
    st.session_state.model_config = config_data["model_config"]
    st.session_state.system_prompt = config_data["system_prompt"]
    st.sidebar.success("ƒê√£ t·∫£i c·∫•u h√¨nh th√†nh c√¥ng!")
    st.rerun()